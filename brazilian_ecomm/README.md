# Brazilian E-Commerce Analytics Pipeline (dbt)

## ğŸ“Œ Contexto do Projeto

Este projeto tem como objetivo construir um **pipeline analÃ­tico completo** utilizando **PostgreSQL + dbt**, partindo de dados brutos locais atÃ© a entrega de **marts analÃ­ticos prontos para consumo**.

A base de dados utilizada Ã© o **Brazilian E-Commerce Public Dataset by Olist**, disponibilizado publicamente no Kaggle:

* Fonte: Kaggle â€“ Brazilian E-Commerce Public Dataset by Olist
* PerÃ­odo: **2016 a 2018**
* Volume: ~100 mil pedidos
* DomÃ­nio: Marketplace / E-commerce

O dataset permite analisar o ciclo completo de um pedido, incluindo:

* Pedidos
* Itens vendidos
* Pagamentos
* Produtos
* Vendedores
* LocalizaÃ§Ã£o geogrÃ¡fica

ğŸ“ Fonte oficial: [https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

---

## ğŸ—ï¸ Arquitetura do Projeto

O projeto segue uma arquitetura clÃ¡ssica de **Analytics Engineering**, separando claramente responsabilidades por camada:

```
ğŸ“¦ Data Sources (CSV local)
      â†“ EL
ğŸ—„ï¸ PostgreSQL (raw tables)
      â†“ T (dbt)
ğŸ§± Staging
      â†“
ğŸ”§ Intermediate
      â†“
ğŸ“Š Marts
```

A modelagem relacional original do dataset pode ser visualizada no diagrama abaixo:

![Diagrama entidade-relacionamento do dataset Olist](docs/images/schema_brazilian_ecomm.png)


---

## âš™ï¸ Tecnologias Utilizadas

* **PostgreSQL** â€“ Banco de dados relacional
* **dbt Core** â€“ TransformaÃ§Ãµes e documentaÃ§Ã£o
* **Python** â€“ Processo de carga (EL)
* **Git / GitHub** â€“ Versionamento
* **VS Code** â€“ Ambiente de desenvolvimento

---

## ğŸš€ Etapas do Projeto

### 1ï¸âƒ£ ExtraÃ§Ã£o (Extract)

Os dados foram baixados diretamente do Kaggle em formato **CSV** e armazenados localmente.

Arquivos utilizados:

* `raw_orders.csv`
* `raw_order_items.csv`
* `raw_order_payments.csv`
* `raw_products.csv`
* `raw_sellers.csv`
* `raw_customers.csv`
* `raw_geolocation.csv`



---

### 2ï¸âƒ£ Carga (Load)

Os arquivos CSV foram carregados no **PostgreSQL** como tabelas raw, preservando o esquema original dos dados.

Boas prÃ¡ticas aplicadas:

* Tipagem adequada de colunas
* Nenhuma regra de negÃ³cio aplicada nesta etapa
* Estrutura preparada para consumo pelo dbt

Resultado:

```
raw.raw__orders
raw.raw_order_items
raw.raw_order_payments
raw.raw_products
raw.raw_sellers
raw.raw_customers
raw.raw_geolocation
```

---

### 3ï¸âƒ£ TransformaÃ§Ã£o (Transform â€“ dbt)

As transformaÃ§Ãµes foram realizadas utilizando **dbt**, organizadas em trÃªs camadas principais:

---

## ğŸ§± Camada Staging

ğŸ¯ Objetivo:

* Limpeza
* PadronizaÃ§Ã£o
* RenomeaÃ§Ã£o de colunas
* AplicaÃ§Ã£o de testes bÃ¡sicos

CaracterÃ­sticas:

* 1 modelo por tabela raw
* Nenhuma agregaÃ§Ã£o
* Granularidade idÃªntica Ã  fonte

Exemplos:

* `stg_orders`
* `stg_order_items`
* `stg_order_payments`
* `stg_products`
* `stg_sellers`

Testes aplicados:

* `not_null`
* `unique`
* `relationships`

---

## ğŸ”§ Camada Intermediate

ğŸ¯ Objetivo:

* Aplicar regras de negÃ³cio
* Realizar joins entre entidades
* Criar mÃ©tricas base reutilizÃ¡veis

CaracterÃ­sticas:

* AgregaÃ§Ãµes controladas
* MÃ©tricas consolidadas
* Modelos reutilizÃ¡veis pelos marts

Principais modelos:

* Vendas por ano
* Vendas por ano-mÃªs
* Vendas por categoria
* Vendas por cidade / estado
* Forma de pagamento
* Vendas acumuladas

Essa camada atua como **fundaÃ§Ã£o analÃ­tica** do projeto.

---

## ğŸ“Š Camada Marts

ğŸ¯ Objetivo:

* Entregar dados prontos para anÃ¡lise
* Responder perguntas de negÃ³cio
* Servir BI, dashboards e stakeholders

CaracterÃ­sticas:

* Modelos finais
* Linguagem de negÃ³cio
* Granularidade explÃ­cita via `meta.grain`

Principais marts criados:

* `marts_forma_pgto`
* `marts_top10_categorias`
* `marts_top10_cidades`
* `marts_top10_estados`
* `marts_venda_ano`
* `marts_venda_ano_mes`
* `marts_venda_acumulada`

---

Linhagem final de todas as camadas criadas:

![Linhagem dos dados](docs/images/dbt-dag_brazilian_ecomm.png)

## ğŸ“ˆ Perguntas de NegÃ³cio Respondidas

Com base nos marts desenvolvidos, o projeto responde perguntas como:

### ğŸ’³ Pagamentos

* Quais sÃ£o as formas de pagamento mais utilizadas?
* Qual a representatividade percentual de cada forma?

### ğŸ›’ Vendas

* Como as vendas evoluÃ­ram ao longo do tempo?
* Qual o acumulado de vendas mÃªs a mÃªs?
* Qual o ticket mÃ©dio por perÃ­odo?

### ğŸ—ºï¸ Geografia

* Quais estados possuem maior volume de vendas?
* Quais cidades concentram as maiores vendas?

### ğŸ“¦ Produtos

* Quais sÃ£o as categorias mais vendidas?
* Quais categorias se destacam em SÃ£o Paulo?
* Como se comportam preÃ§o mÃ©dio e volume por categoria?

---

## ğŸ“š DocumentaÃ§Ã£o

Toda a documentaÃ§Ã£o foi gerada utilizando **dbt docs**, incluindo:

* DescriÃ§Ã£o de modelos
* DescriÃ§Ã£o de colunas
* Granularidade (`meta.grain`)
* Camada (`meta.layer`)

Para visualizar:

```bash
dbt docs generate
dbt docs serve
```

---

## ğŸ§  ConclusÃ£o

Este projeto demonstra um pipeline analÃ­tico completo, seguindo boas prÃ¡ticas de:

* Modelagem analÃ­tica
* Analytics Engineering
* GovernanÃ§a de dados
* DocumentaÃ§Ã£o orientada a negÃ³cio

Ele pode ser facilmente estendido para:

* Dashboards em Power BI / Looker / Tableau
* IntegraÃ§Ã£o com novas fontes
* Casos avanÃ§ados de analytics e ciÃªncia de dados

---

ğŸ“Œ **Autor:** Raphael Pylypiec
ğŸ“Œ **Stack:** PostgreSQL â€¢ dbt â€¢ Python
