# Brazilian E-Commerce Analytics Pipeline (dbt)

## üìå Contexto do Projeto

Este projeto tem como objetivo construir um **pipeline anal√≠tico completo** utilizando **PostgreSQL + dbt**, partindo de dados brutos locais at√© a entrega de **marts anal√≠ticos prontos para consumo**.

A base de dados utilizada √© o **Brazilian E-Commerce Public Dataset by Olist**, disponibilizado publicamente no Kaggle:

* Fonte: Kaggle ‚Äì Brazilian E-Commerce Public Dataset by Olist
* Per√≠odo: **2016 a 2018**
* Volume: ~100 mil pedidos
* Dom√≠nio: Marketplace / E-commerce

O dataset permite analisar o ciclo completo de um pedido, incluindo:

* Pedidos
* Itens vendidos
* Pagamentos
* Produtos
* Vendedores
* Localiza√ß√£o geogr√°fica

üìé Fonte oficial: [https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

---

## üèóÔ∏è Arquitetura do Projeto

O projeto segue uma arquitetura cl√°ssica de **Analytics Engineering**, separando claramente responsabilidades por camada:

```
üì¶ Data Sources (CSV local)
      ‚Üì EL
üóÑÔ∏è PostgreSQL (raw tables)
      ‚Üì T (dbt)
üß± Staging
      ‚Üì
üîß Intermediate
      ‚Üì
üìä Marts
```

A modelagem relacional original do dataset pode ser visualizada no diagrama abaixo:

![Diagrama entidade-relacionamento do dataset Olist](docs/images/schema_brazilian_ecomm.png)


---

## ‚öôÔ∏è Tecnologias Utilizadas

* **PostgreSQL** ‚Äì Banco de dados relacional
* **dbt Core** ‚Äì Transforma√ß√µes e documenta√ß√£o
* **Python** ‚Äì Processo de carga (EL)
* **Airflow** ‚Äì Orquestra√ß√£o do pipeline
* **Docker / Docker Compose** ‚Äì Isolamento de ambiente
* **Git / GitHub** ‚Äì Versionamento
* **VS Code** ‚Äì Ambiente de desenvolvimento

---

## üöÄ Etapas do Projeto

### 1Ô∏è‚É£ Extra√ß√£o (Extract)

Os dados foram baixados diretamente do Kaggle em formato **CSV** e armazenados localmente.

Arquivos utilizados:

* `raw_orders.csv`
* `raw_order_items.csv`
* `raw_order_payments.csv`
* `raw_products.csv`
* `raw_sellers.csv`
* `raw_customers.csv`
* `raw_geolocation.csv`



---

### 2Ô∏è‚É£ Carga (Load)

Os arquivos CSV foram carregados no **PostgreSQL** como tabelas raw, preservando o esquema original dos dados.

Boas pr√°ticas aplicadas:

* Tipagem adequada de colunas
* Nenhuma regra de neg√≥cio aplicada nesta etapa
* Estrutura preparada para consumo pelo dbt

Resultado:

```
raw.raw__orders
raw.raw_order_items
raw.raw_order_payments
raw.raw_products
raw.raw_sellers
raw.raw_customers
raw.raw_geolocation
```

---

### 3Ô∏è‚É£ Transforma√ß√£o (Transform ‚Äì dbt)

As transforma√ß√µes foram realizadas utilizando **dbt**, organizadas em tr√™s camadas principais:

---

## üß± Camada Staging

üéØ Objetivo:

* Limpeza
* Padroniza√ß√£o
* Renomea√ß√£o de colunas
* Aplica√ß√£o de testes b√°sicos

Caracter√≠sticas:

* 1 modelo por tabela raw
* Nenhuma agrega√ß√£o
* Granularidade id√™ntica √† fonte

Exemplos:

* `stg_orders`
* `stg_order_items`
* `stg_order_payments`
* `stg_products`
* `stg_sellers`

Testes aplicados:

* `not_null`
* `unique`
* `relationships`

---

## üîß Camada Intermediate

üéØ Objetivo:

* Aplicar regras de neg√≥cio
* Realizar joins entre entidades
* Criar m√©tricas base reutiliz√°veis

Caracter√≠sticas:

* Agrega√ß√µes controladas
* M√©tricas consolidadas
* Modelos reutiliz√°veis pelos marts

Principais modelos:

* Vendas por ano
* Vendas por ano-m√™s
* Vendas por categoria
* Vendas por cidade / estado
* Forma de pagamento
* Vendas acumuladas

Essa camada atua como **funda√ß√£o anal√≠tica** do projeto.

---

## üìä Camada Marts

üéØ Objetivo:

* Entregar dados prontos para an√°lise
* Responder perguntas de neg√≥cio
* Servir BI, dashboards e stakeholders

Caracter√≠sticas:

* Modelos finais
* Linguagem de neg√≥cio
* Granularidade expl√≠cita via `meta.grain`

Principais marts criados:

* `marts_forma_pgto`
* `marts_top10_categorias`
* `marts_top10_cidades`
* `marts_top10_estados`
* `marts_venda_ano`
* `marts_venda_ano_mes`
* `marts_venda_acumulada`

---

Linhagem final de todas as camadas criadas:

![Linhagem dos dados](docs/images/dbt-dag_brazilian_ecomm.png)

---

### üîπ Diagrama Atualizado do Pipeline
```
+-------------------+       +--------------------+       +--------------------+
|  CSV Local Files  |  EL   |   PostgreSQL Raw   |  T    |      dbt Models    |
| orders, products, | ----> |  raw tables        | ----> | Staging /          |
| payments, etc.    |       |                    |       | Intermediate / Marts|
+-------------------+       +--------------------+       +--------------------+
           |                          |                        |
           |                          |                        |
           v                          v                        v
   +-------------------+       +--------------------+       +--------------------+
   |  Docker Compose   |       |    Airflow DAGs    |       |    dbt Docs        |
   | Postgres / Airflow|<----->| orchestrating EL+T |       | HTML Documentation |
   |  Volumes / Logs   |       |                    |       |                    |
   +-------------------+       +--------------------+       +--------------------+
```
--

## üìà Perguntas de Neg√≥cio Respondidas

Com base nos marts desenvolvidos, o projeto responde perguntas como:

### üí≥ Pagamentos

* Quais s√£o as formas de pagamento mais utilizadas?
* Qual a representatividade percentual de cada forma?

### üõí Vendas

* Como as vendas evolu√≠ram ao longo do tempo?
* Qual o acumulado de vendas m√™s a m√™s?
* Qual o ticket m√©dio por per√≠odo?

### üó∫Ô∏è Geografia

* Quais estados possuem maior volume de vendas?
* Quais cidades concentram as maiores vendas?

### üì¶ Produtos

* Quais s√£o as categorias mais vendidas?
* Quais categorias se destacam em S√£o Paulo?
* Como se comportam pre√ßo m√©dio e volume por categoria?

--

## üìö Documenta√ß√£o

Toda a documenta√ß√£o foi gerada utilizando **dbt docs**, incluindo:

* Descri√ß√£o de modelos
* Descri√ß√£o de colunas
* Granularidade (`meta.grain`)
* Camada (`meta.layer`)

Para visualizar:

```bash
dbt docs generate
dbt docs serve
```

---

## üíª Como Rodar o Pipeline Localmente

Siga estas instru√ß√µes para rodar **o pipeline completo** na sua m√°quina, incluindo PostgreSQL, Airflow, EL com Python e T com dbt.

---

### 1Ô∏è‚É£ Pr√©-requisitos

Antes de come√ßar, instale os seguintes softwares:

* [Docker](https://www.docker.com/get-started)
* [Docker Compose](https://docs.docker.com/compose/install/)
* [Git](https://git-scm.com/)
* (Opcional) VS Code ou outro editor de c√≥digo

> Certifique-se de que as portas 5432 (Postgres), 8080 (Airflow) e 8081 (dbt docs) est√£o livres.

---

### 2Ô∏è‚É£ Clonar o reposit√≥rio

```bash
git clone https://github.com/seu-usuario/brazilian_ecomm_dw.git
cd brazilian_ecomm_dw
```

### 3Ô∏è‚É£ Configurar vari√°veis e credenciais

* Se existir, copie .env.example para .env:

```bash
cp .env.example .env
```

### 4Ô∏è‚É£ Subir o ambiente Docker

```bash
docker compose up -d
```

Isso vai criar e iniciar:

* PostgreSQL (banco de dados)

* Airflow (scheduler, webserver e worker)

* Volumes persistentes para dados e logs

Verifique se os containers est√£o rodando:

```bash
docker ps
```

### 5Ô∏è‚É£ Acessar Airflow

* Abra http://localhost:8080 no navegador

* Usu√°rio/senha padr√£o: airflow / airflow

* DAG principal: brazilian_ecomm_pipeline

* √â poss√≠vel executar manualmente ou configurar agendamento

### 6Ô∏è‚É£ Executar dbt dentro do container

Entre no container do Airflow:

```bash
docker exec -it airflow-webserver bash
cd /opt/airflow/brazilian_ecomm
```

Execute os comandos dbt:

```bash
# Rodar todas as transforma√ß√µes
dbt run

# Gerar documenta√ß√£o
dbt docs generate

# Servir documenta√ß√£o via web
dbt docs serve --port 8081
```

* A documenta√ß√£o estar√° dispon√≠vel em http://localhost:8081

* Caso a porta 8081 j√° esteja em uso, escolha outra porta:

```bash
dbt docs serve --port 8082
```

### 7Ô∏è‚É£ Parar e limpar o ambiente

Para parar os containers e liberar recursos:

```bash
docker compose down
```

Para remover volumes e dados persistentes (opcional):

```bash
docker compose down -v
```

### üîß Dicas √∫teis

* Logs do Airflow:

```bash
docker logs -f airflow-webserver
docker logs -f airflow-scheduler
```

* Se houver conflitos de portas, altere no docker-compose.yml ou no comando dbt.

* Sempre garanta que os dados CSV estejam na pasta data/raw antes de rodar o EL.

## üß† Conclus√£o

Este projeto demonstra um pipeline anal√≠tico completo, seguindo boas pr√°ticas de:

* Modelagem anal√≠tica
* Analytics Engineering
* Governan√ßa de dados
* Documenta√ß√£o orientada a neg√≥cio

Ele pode ser facilmente estendido para:

* Dashboards em Power BI / Looker / Tableau
* Integra√ß√£o com novas fontes
* Casos avan√ßados de analytics e ci√™ncia de dados

---

üìå **Autor:** Raphael Pylypiec
üìå **Stack:** PostgreSQL ‚Ä¢ dbt ‚Ä¢ Python ‚Ä¢ Airflow ‚Ä¢ Docker
